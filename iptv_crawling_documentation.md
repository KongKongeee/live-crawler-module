# Iptv_crawling 기술 문서

## 1. 프로젝트 개요

IPTV 편성표 정보 크롤링 및 메타데이터 수집 자동화 프로젝트

**목표:**

*   LGU+ IPTV 웹사이트에서 특정 채널들의 일일 편성표 정보를 크롤링합니다.
*   수집된 프로그램 정보를 바탕으로 TMDB, Naver 검색, Gemini API 등 다양한 소스를 활용하여 상세 메타데이터(장르, 줄거리, 포스터, 출연진 등)를 보강합니다.
*   수집된 데이터를 정제하고 구조화하여 CSV 파일로 저장합니다.
*   메타데이터 수집 비용과 시간을 절약하기 위해 이전에 수집한 정보를 캐싱하고 재사용합니다.

**주요 기술:**

*   **Crawling:** Selenium, BeautifulSoup4
*   **Data Handling:** Pandas
*   **Metadata Sources:**
    *   TMDB API
    *   Naver Search (via Selenium)
    *   Google Gemini API
*   **Concurrency:** `concurrent.futures.ThreadPoolExecutor`
*   **Configuration:** `python-dotenv`

## 2. 시스템 아키텍처

```
+-------------------+      +----------------------+      +---------------------+
|                   |      |                      |      |                     |
|   main.py         +------>   Crawler (modules)  +------>  MetadataManager    |
| (실행 스크립트)     |      | (크롤링 및 데이터 처리) |      | (메타데이터 수집/관리) |
|                   |      |                      |      |                     |
+-------------------+      +----------+-----------+      +----------+----------+
                                      |                         |
                                      |                         |
           +--------------------------+-------------------------+
           |                          |                         |
+----------v-----------+   +----------v----------+   +----------v----------+
|                      |   |                     |   |                     |
|  TMDB API (tmdb.py)  |   | Naver (naver.py)    |   | Gemini (gemini.py)  |
|                      |   |                     |   |                     |
+----------------------+   +---------------------+   +---------------------+
```

**흐름:**

1.  `main.py`: `Crawler` 클래스를 인스턴스화하고 `run()` 메소드를 호출하여 전체 프로세스를 시작합니다.
2.  `Crawler`:
    *   Selenium을 사용하여 LGU+ IPTV 편성표 페이지에 접근합니다.
    *   `ThreadPoolExecutor`를 통해 지정된 채널 목록을 병렬로 크롤링합니다.
    *   각 채널의 편성표 HTML을 파싱하여 프로그램명, 방송 시간, 원본 장르 등의 기본 정보를 추출합니다.
    *   추출된 기본 정보를 `MetadataManager`에 전달하여 상세 메타데이터를 요청합니다.
    *   수집된 모든 데이터를 취합하고 정제하여 최종 CSV 파일로 저장합니다.
    *   수집된 메타데이터를 `metadata_cache.csv`에 업데이트하여 다음 실행 시 재사용할 수 있도록 합니다.
3.  `MetadataManager`:
    *   프로그램명을 입력받아 가장 적합한 메타데이터를 반환합니다.
    *   **캐시 우선 조회:** `metadata_cache.csv`에 해당 프로그램의 정보가 있는지 먼저 확인합니다.
    *   **TMDB API 조회:** 캐시에 정보가 없으면 TMDB API를 통해 영화/TV 쇼 정보를 검색합니다.
    *   **Naver 검색 보강:** TMDB에서 정보를 찾지 못하거나 부족할 경우, Naver 검색을 통해 추가 정보(장르, 썸네일, 출연진)를 수집합니다.
    *   **Gemini API 활용:** 위 단계들로도 정보가 부족할 경우(예: 줄거리, 서브 장르, 연령 등급 누락), Gemini API에 프로그램명과 현재까지 수집된 정보를 전달하여 누락된 필드를 추론하고 채웁니다.
    *   **장르 정제:** `genre_config.py`에 정의된 규칙에 따라 원본 장르(예: '연예/오락')를 표준 장르('예능')로 매핑하고, 프로그램 설명과 키워드를 분석하여 가장 적절한 서브 장르를 추론 및 검증합니다.
4.  **Configuration (`genre_config.py`):**
    *   장르 매핑 규칙, 서브 장르 목록, 키워드 등 메타데이터 정제에 필요한 모든 설정을 관리합니다. 이를 통해 코드 변경 없이 설정 파일 수정만으로 장르 분류 기준을 유연하게 변경할 수 있습니다.

## 3. 핵심 알고리즘 및 데이터 처리 방식

### 3.1. 데이터 전처리 및 정제

수집된 원본 데이터의 품질과 일관성을 높이고, 정확한 메타데이터 매칭을 위해 여러 단계의 전처리 과정을 수행합니다.

**1. 프로그램명 정제 (`lib/utils/text_cleaning.py`의 `clean_name`):**
메타데이터 검색 정확도를 높이기 위해 원본 프로그램명에서 불필요한 정보를 제거합니다.
*   **특수 기호 및 괄호 제거:** `()`, `[]`, `<>`, `〈〉` 등 모든 종류의 괄호와 그 안의 내용을 제거합니다.
*   **방송 관련 키워드 제거:** '재방송', '특별판', '본방송', 'HD', 'NEW' 등 방송 상태를 나타내는 단어를 제거하여 핵심 제목만 남깁니다.
*   **회차 및 부제 정보 제거:** `125회`, `3부` 와 같은 회차 정보를 정규식을 통해 제거합니다.
*   **공백 및 특수문자 정규화:** 불필요한 공백과 특수문자(`“”:|-·,` 등)를 표준 공백으로 변환하여 일관성을 유지합니다.

**2. 회차 정보 추출 (`modules/crawler.py`):**
*   `clean_name`으로 제목을 정제하기 전에, `(\d{1,4}회)` 정규식을 사용하여 원본 제목에서 '12회'와 같은 회차 정보를 미리 추출하여 별도의 'episode' 필드에 저장합니다.

**3. 방송 시간(Runtime) 계산 (`modules/crawler.py`의 `calculate_runtime`):**
*   편성표에는 각 프로그램의 시작 시간만 명시되어 있으므로, 현재 프로그램의 시작 시간과 바로 다음 프로그램의 시작 시간의 차이를 분 단위로 계산하여 `runtime`을 결정합니다.
*   자정을 넘어가는 편성(예: 23:30 ~ 00:30)도 날짜를 하루 더하는 방식으로 정확하게 처리합니다.

**4. 데이터 표준화 (`modules/crawler.py` 및 `lib/metadata/metadata_manager.py`):**
*   **결측치 처리:** '정보 없음'과 같은 문자열을 `NaN`으로 일괄 변환하여 데이터 분석에 용이하도록 합니다.
*   **연령 등급 표준화:** '12', '15', 'ALL', '전체' 등 다양한 형식의 연령 등급을 '12세 이상', '15세 이상', '전체 이용가' 와 같은 일관된 형식으로 통일합니다.
*   **장르 매핑:** '연예/오락' -> '예능', '만화' -> '애니' 와 같이 `genre_map`에 정의된 규칙에 따라 원본 장르명을 표준 장르명으로 변환합니다.

### 3.2. 메타데이터 수집 및 보강 전략 (캐싱 및 다중 소스 활용)

본 시스템의 핵심은 **계층적 데이터 수집 전략**과 **캐싱**을 통해 정확성과 효율성을 동시에 확보하는 것입니다.

1.  **1단계: 로컬 캐시 확인 (`metadata_cache.csv`)**
2.  **2단계: TMDB API 우선 조회**
3.  **3단계: Naver 웹 검색을 통한 보강**
4.  **4단계: Gemini API를 이용한 최종 보완**

### 3.3. 장르 및 서브 장르 정제 로직 (`validate_and_fix_subgenre`)

정확한 장르 분류는 데이터 활용의 핵심 요소이므로, 다음과 같은 다단계 정제 로직을 사용합니다.

1.  **1차: 허용 목록 기반 검증**
2.  **2차: 프로그램 설명 기반 추론 (`guess_subgenre_by_desc`)**
3.  **3차: 키워드 기반 폴백(Fallback)**
4.  **최종 정리:** '다큐' -> '예능-다큐멘터리' 등으로 최종 변환

### 3.4. 프로그램 ID 연속성 보장 (`get_last_program_id_by_yesterday`)

*   일일 편성표 데이터를 고유하게 식별하기 위해 `program_id`를 사용합니다.
*   크롤러는 실행 시 어제 날짜로 저장된 CSV 파일을 확인하여 가장 마지막 `program_id`를 가져옵니다.
*   오늘 수집한 데이터의 `program_id`는 어제의 마지막 ID에 이어서 1씩 증가하는 값으로 부여됩니다.

### 3.5. 병렬 처리를 통한 성능 최적화

*   `ThreadPoolExecutor`를 사용하여 다수의 채널 편성표를 동시에 크롤링하고 메타데이터를 수집합니다.
*   `max_workers` 파라미터를 통해 시스템 환경에 맞게 동시 작업 스레드 수를 조절할 수 있습니다.

## 4. 실행 방법 및 요구사항

1.  **필수 라이브러리 설치:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **환경 변수 설정:**
    *   `.env` 파일에 TMDB 및 Gemini API 키를 설정해야 합니다.
    ```
    TMDB_API_KEY="YOUR_TMDB_API_KEY"
    GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
    ```

3.  **실행:**
    ```bash
    python main.py
    ```

4.  **결과물:**
    *   `data_crawling_tmdb_gemini/` 디렉토리에 `YYYY-MM-DD_실시간_방영_프로그램_리스트.csv` 형식으로 최종 데이터가 저장됩니다.
    *   `cache/metadata_cache.csv` 파일이 생성되거나 업데이트됩니다.

## 5. 디렉토리 구조

```
ifitv_cralwer/
├── .env                  # API 키 등 환경 변수
├── main.py               # 메인 실행 파일
├── requirements.txt      # 의존성 목록
├── modules/
│   └── crawler.py        # 핵심 크롤러 로직
├── lib/
│   ├── config/
│   │   ├── genre_config.py # 장르 매핑 및 분류 규칙
│   │   └── metadata_exceptions.json # 메타데이터 예외 처리
│   ├── metadata/
│   │   ├── metadata_manager.py # 메타데이터 수집/관리 총괄
│   │   ├── tmdb.py         # TMDB API 연동
│   │   ├── naver.py        # Naver 검색 로직
│   │   └── gemini.py       # Gemini API 연동
│   └── utils/
│       └── text_cleaning.py # 텍스트 정제 유틸리티
├── data_crawling_tmdb_gemini/ # 최종 결과 CSV 저장소
└── cache/
    └── metadata_cache.csv  # 메타데이터 캐시
```
